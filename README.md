# âœˆï¸ FlightRadar24 - ETL Pipeline avec CRONJOB et Analyse Pyspark

## Objectif

Ce projet vise Ã  construire un pipeline **ETL industrialisÃ©**, **tolÃ©rant aux erreurs**, et **observable**, qui rÃ©cupÃ¨re toutes les **2 heures** les donnÃ©es de vol en temps rÃ©el depuis lâ€™API [FlightRadar24](https://github.com/JeanExtreme002/FlightRadarAPI), les nettoie, les stocke au format **CSV** (nomenclature horodatÃ©e)(ou parquet), puis lance une **analyse Spark** pour gÃ©nÃ©rer des **indicateurs mÃ©tier** sur le trafic aÃ©rien mondial.

---

## âš™ï¸ Architecture du pipeline

```
+----------------------+     +----------------------+     +--------------------------+
|     EXTRACT          | --> |     TRANSFORM        | --> |        LOAD              |
|  API FlightRadar24   |     |  Nettoyage, EDA      |     |  CSV (Parquet sur la V1) |
+----------------------+     +----------------------+     +--------------------------+

                          Orchestration toutes les 2h

                                 |
                                 v

                            +-------------+
                            |  PySpark    |
                            |  Analyses   |
                            +-------------+
```

---

## Structure du projet

```
Flight_radar_ETL/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py              # RÃ©cupÃ©ration des vols depuis FlightRadar24
â”‚   â”œâ”€â”€ transform.py            # Exploration et nettoyage des donnÃ©es
â”‚   â”œâ”€â”€ load.py                 # Sauvegarde CSV horodatÃ©e
â”‚   â”œâ”€â”€ pipeline.py             # Orchestration ETL en script Python
â”‚   â”œâ”€â”€ Cronjob/                # Jobs CRON pour exÃ©cuter le pipeline automatiquement
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ job_2H.py           # Job CRON exÃ©cutÃ© toutes les 2 heures
â”‚   â”‚   â””â”€â”€ job_test_2min.py    # Job de test exÃ©cutÃ© toutes les 2 minutes
â”‚   â””â”€â”€ Flights/                # pour avori de la data sous format parquet 
â”‚       
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ rawzone/                # DonnÃ©es CSV structurÃ©es : tech_year=YYYY/...
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Flight_radar_ETL.ipynb  # Notebook principal pour affichage des rÃ©sultats
â”‚
â”œâ”€â”€ spark_analysis.py          # Script PySpark pour les analyses (KPIs)
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python (FlightRadarAPI, pandas, etc.)
â”œâ”€â”€ readme.md                  # Documentation du projet
â”œâ”€â”€ conception.md              # SchÃ©ma dâ€™architecture
â”œâ”€â”€ LICENSE
â””â”€â”€ Dockerfile                 # Conteneurisation du pipeline si besoin
```

---

## Stack technique

- **Python** 3.9.5
- **FlightRadar24 API** (librairie `FlightRadarAPI`)
- **pandas** (EDA + nettoyage)
- **CRON Python** (orchestration via boucle infinie avec `sleep`)
- **Airflow** (orchestration toutes les 2h) ( AbondonnÃ© pour le moment)
- **Parquet** (stockage optimisÃ©)  ( AbondonnÃ© pour le moment)
- **PySpark** (analyses distribuÃ©es)(3.5.0)
- **Logging Python** (observabilitÃ©)


## ğŸ” Orchestration avec CRON Python (Job 2H)

Un script Python (`pipeline.py`) encapsule le pipeline complet et est lancÃ© automatiquement toutes les **2 heures** via une boucle ou tÃ¢che planifiÃ©e (cronjob).  
Pour test local, on peut dÃ©finir `sleep(120)` (2 minutes).

Exemple :
```bash
python pipeline.py  # Lancement unique
```
Ou dans une boucle infinie :

```
while True:
    run_pipeline()
    time.sleep(2 * 60 * 60)  # 2 heures
```
---

## ğŸ” Orchestration via Airflow ( Pas utilisÃ© pour le moment )

Le fichier `scheduler/flightradar_dag.py` dÃ©finit un DAG Airflow dÃ©clenchÃ© toutes les **2 heures**, composÃ© de 3 tÃ¢ches :
- `extract_task` â†’ `transform_task` â†’ `load_task`

L'exÃ©cution du pipeline gÃ©nÃ¨re un fichier Parquet partitionnÃ© par :
```
data/rawzone/tech_year=YYYY/tech_month=YYYY-MM/tech_day=YYYY-MM-DD/
```
---

## Indicateurs mÃ©tier calculÃ©s

Les fichiers gÃ©nÃ©rÃ©s sont analysÃ©s via **PySpark** dans `spark_analysis.py`.  
Voici les **indicateurs extraits** :

1. Compagnie avec le plus de vols en cours

2. Par continent, la compagnie avec le plus de vols rÃ©gionaux

3. Vol en cours avec le trajet le plus long

4.  Moyenne des distances de vol par continent

5.  Constructeur dâ€™avions avec le plus de vols actifs

6.  Top 3 modÃ¨les dâ€™avions utilisÃ©s par pays de la compagnie

---

## Stockage horodatÃ©

Les fichiers CSV sont sauvegardÃ©s dans la structure suivante :

```bash

Flights/rawzone/
  â””â”€â”€ tech_year=2025/
        â””â”€â”€ tech_month=2025-07/
              â””â”€â”€ tech_day=2025-07-13/
                    â””â”€â”€ flights_20250713122027.csv
```

## TolÃ©rance aux erreurs

- Bloc try/except autour de chaque Ã©tape (extract, transform, load)
- Les messages apparaissent aussi dans le terminal ou le notebook
- Les erreurs Spark sont Ã©galement loggÃ©es

## Comment exÃ©cuter le pipeline manuellement

```bash
# Lancer manuellement le pipeline ETL
python etl/pipeline.py

# Lancer l'analyse Spark
python spark_analysis.py
```

Ou depuis le notebook :

```bash

run_pipeline()
```
---

## ObservabilitÃ©

- Les logs dâ€™extraction, de transformation, de nettoyage et de sauvegarde sont disponibles Ã  chaque run.
- **Les logs Airflow permettent une visibilitÃ© complÃ¨te de lâ€™exÃ©cution**. ( Ou les logs du Cronjob python)

---

## FrÃ©quence de mise Ã  jour

Le pipeline est dÃ©clenchÃ© **toutes les 2 heures** pour capter les nouvelles positions des vols, et maintenir des analyses **Ã  jour en quasi temps rÃ©el**.

---

## Comment exÃ©cuter le pipeline manuellement

```bash
# Lancer manuellement l'ETL
python main.py

# Lancer les analyses PySpark
spark-submit notebooks/spark_analysis.py

## ou bien 
python spark_analysis.py
```
Ou bien lancer le notebook Flight_radar_ETL
---

## AmÃ©liorations possibles

- Utilisation d'un systÃ©me de stockage en base de donnÃ©es ( postgre par ex) et remplacer le Cronjob Python par un orchestrateur comme Airflow
- Dashboard en live via **Grafana** ou **Tableau** 
- Monitoring via Grafana / Prometheus

---

## Logique dâ€™exÃ©cution

1. CRON dÃ©clenche le job pipeline.py toutes les 2 heures

2. Le pipeline :

      - extrait les donnÃ©es de lâ€™API

      - nettoie avec pandas

      - sauvegarde en CSV dans Flights/rawzone/...


3. Le script spark_analysis.py lit le dernier fichier CSV et affiche les rÃ©sultats mÃ©tiers


## Remarques

- Jâ€™ai optÃ© pour un cronjob Python temporaire Ã  la place dâ€™Airflow, en raison de contraintes de compatibilitÃ© (notamment avec WSL).  
  Une version orchestrÃ©e via Airflow sera proposÃ©e dans une future branche.

- Jâ€™ai utilisÃ© des fichiers CSV dans cette premiÃ¨re version afin d'observer concrÃ¨tement les diffÃ©rences avec le format Parquet : taille des fichiers, vitesse de traitement, intÃ©gration avec Spark, etc.  
  Le format Parquet, orientÃ© colonne, permet une rÃ©duction significative de la taille des fichiers, un accÃ¨s plus rapide aux donnÃ©es lors des agrÃ©gations, ainsi quâ€™un meilleur support des types de donnÃ©es.  
  Une version optimisÃ©e du pipeline, exploitant le format Parquet, sera proposÃ©e dans une prochaine branche.


## Authored By me 

N'hÃ©sitez pas Ã  me contacter sur LinkedIn en cas de problÃ¨me ou piste d'amÃ©lioration.

![alt text](image.png)
